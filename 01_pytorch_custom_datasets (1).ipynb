{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJd39W1JFvhI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "ENxQM34FF7Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory already exists\")\n",
        "else:\n",
        "    print(f\"Creating {image_path} directory\")\n",
        "    image_path.mkdir(parents = True, exist_ok= True)\n",
        "\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/data/pizza_steak_sushi.zip\")\n",
        "  print(\"Downloading pizza, steak, sushi data\")\n",
        "  f.write(request.content)\n",
        "\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\")as zip_ref:\n",
        "  print(\"Unzipping pizza, steak, sushi data\")\n",
        "  zip_ref.extractall(image_path)"
      ],
      "metadata": {
        "id": "ym9vLXuKF-hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are{len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "nc3Ygs_KGBYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "iGMYTvaPGDzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "random_image_path = random.choice(image_path_list)\n",
        "image_class = random_image_path.parent.stem\n",
        "img = Image.open(random_image_path)\n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"The image is of class: {image_class}\")\n",
        "print(f\"The image height is: {img.height}\")\n",
        "print(f\"The image width is: {img.width}\")\n",
        "img\n"
      ],
      "metadata": {
        "id": "mVInmqQwXd_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_as_array = np.asarray(img)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(img_as_array)\n",
        "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "_0vP16YlXlks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets,transforms"
      ],
      "metadata": {
        "id": "D4z_uI2cGZOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.ImageFolder(root = train_dir,\n",
        "                                  )\n",
        "test_data = datasets.ImageFolder(root = test_dir,\n",
        "                                  )\n",
        "train_data, test_data"
      ],
      "metadata": {
        "id": "k3mWecjGJ1qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_transform_0 = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "LH1PDsi1YAvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.classes,train_data.class_to_idx"
      ],
      "metadata": {
        "id": "5m3xFEDLKHRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "pwsrhD_MEHH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])"
      ],
      "metadata": {
        "id": "KZZT4f8PGm5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "train_data_simple = datasets.ImageFolder(root=train_dir,\n",
        "                                         transform=simple_transform,\n",
        "                                         target_transform=None)\n",
        "test_data_simple = datasets.ImageFolder(root = test_dir,\n",
        "                                         transform=simple_transform,\n",
        "                                         target_transform=None)\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "train_dataloader_simple = DataLoader(dataset=train_data_simple,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              shuffle=True)\n",
        "test_dataloader_simple = DataLoader(dataset=test_data_simple,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              shuffle=False)\n",
        "train_dataloader_simple, test_dataloader_simple"
      ],
      "metadata": {
        "id": "67H2RLUvGxHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = next(iter(train_dataloader_simple))\n",
        "print({img.shape})\n",
        "print({label.shape})"
      ],
      "metadata": {
        "id": "aPhnpDKXKqTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"\n",
        "  Model architecture that replicates the TinyVGG\n",
        "  model from CNN explainer website.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_shape,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.BatchNorm2d(hidden_units),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.BatchNorm2d(hidden_units),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels = hidden_units,\n",
        "                out_channels = hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.BatchNorm2d(hidden_units),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels = hidden_units,\n",
        "                out_channels = hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.BatchNorm2d(hidden_units),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Dropout(p=0.5),\n",
        "      nn.Linear(in_features=hidden_units*16*16,\n",
        "                out_features=output_shape)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
      ],
      "metadata": {
        "id": "Wi4JEVoDHBES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_0 = TinyVGG(input_shape =3,\n",
        "                   hidden_units =32,\n",
        "                   output_shape = len(class_names))\n",
        "model_0"
      ],
      "metadata": {
        "id": "rU4ZyYkIHJgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "  import torchinfo\n",
        "from torchinfo import summary\n",
        "summary(model_0,input_size=(1,3,64,64))"
      ],
      "metadata": {
        "id": "4xJOC07PHS1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_step(\n",
        "    model: torch.nn.Module,\n",
        "    data_loader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    accuracy_fn,\n",
        "    device: torch.device = 'cpu',\n",
        "):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_pred = model(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    #print(f\"Train loss: {train_loss : .5f}  |  Train acc: {train_acc: .2f}%\")\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "QwaSaCqLMFL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(\n",
        "    model: torch.nn.Module,\n",
        "    data_loader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    accuracy_fn,\n",
        "    device: torch.device = 'cpu',\n",
        "):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            test_pred = model(X)\n",
        "            test_loss += loss_fn(test_pred, y).item()\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        #print(f\"Test loss: {test_loss: .4f} | Test acc: {test_acc: .4f}\")\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "r0yenNfDHcMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(\n",
        "    model: torch.nn.Module,\n",
        "    data_loader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    accuracy_fn,\n",
        "    device: torch.device = 'cpu',\n",
        "):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            test_pred = model(X)\n",
        "            test_loss += loss_fn(test_pred, y).item()\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss: .4f} | Test acc: {test_acc: .4f}\")\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "xKPtzezVHeuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n",
        "\n",
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    test_dataloader: torch.utils.data.dataloader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "    epochs: int = 5,\n",
        "    device: torch.device = device,\n",
        "):\n",
        "    results = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(\n",
        "            model=model,\n",
        "            data_loader=train_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device,\n",
        "        )\n",
        "        test_loss, test_acc = test_step(\n",
        "            model=model,\n",
        "            data_loader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device,\n",
        "        )\n",
        "        print(\n",
        "            f\"Epoch:{epoch} | Train loss: {train_loss: .4f} | Train acc:{train_acc: .4f} | Test loss:{test_loss: .4f} | Test acc: {test_acc: .4f}\"\n",
        "        )\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "    return results"
      ],
      "metadata": {
        "id": "-K26YMIbHgkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 25\n",
        "model_0 = TinyVGG(input_shape=3,\n",
        "                   hidden_units=32,\n",
        "                   output_shape=len(train_data.classes))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(),\n",
        "                             lr = 0.0003,\n",
        "                             weight_decay=1e-4)\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "model_0_results = train(model=model_0,\n",
        "                        train_dataloader=train_dataloader_simple,\n",
        "                        test_dataloader=test_dataloader_simple,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        device=device\n",
        "                        )\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "-Oy-NY3WHpl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "BcoHs6nlIZUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "  \"\"\" Plots training curves of a results dictionary. \"\"\"\n",
        "  loss = results[\"train_loss\"]\n",
        "  test_loss = results[\"test_loss\"]\n",
        "  accuracy = results[\"train_acc\"]\n",
        "  test_accuracy = results[\"test_acc\"]\n",
        "  epochs = range(len(results[\"train_loss\"]))\n",
        "  plt.figure(figsize=(15, 7))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs, loss, label=\"train_loss\")\n",
        "  plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
        "  plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "pHVyYJQHIeX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_0_results)"
      ],
      "metadata": {
        "id": "a7YRUtMlN9i1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}